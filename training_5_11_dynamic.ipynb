{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoffLund/Neural-network/blob/main/training_5_11_dynamic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvS68CiOp0Tn"
      },
      "source": [
        "#Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UIT73_L6p6zb"
      },
      "outputs": [],
      "source": [
        "import os                           # used for clearing the save folder when a new save is created\n",
        "import numpy as np                  # used for the matrix operations and matrix structure\n",
        "import time                         # used to track the training time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPDUs0_Zwpdu"
      },
      "source": [
        "# Cloning git repository (to have training data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXU4yPuGwtEc",
        "outputId": "74764282-2c9d-4c04-f622-044bc9e3c42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Neural-network' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HoffLund/Neural-network.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0No8arqqz9N"
      },
      "source": [
        "#Functions for saving neural network and loading image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "66vD31GJq4bb"
      },
      "outputs": [],
      "source": [
        "# Saves the neural networks layers to be loaded in a new session or to be used as checkpoints.\n",
        "# Each weightlayer and biaslayer gets saved in a seperate file\n",
        "def savenetwork(printpath):\n",
        "    folder_path = os.path.join(currentpath,\"saved_network/\")\n",
        "    for file in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        if printpath == True:\n",
        "            print(\"Deleting:\",file_path)\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)  # Delete the file\n",
        "\n",
        "    for name in namelist():\n",
        "        layer = globals()[name]\n",
        "        filesave(name,layer,printpath)\n",
        "\n",
        "# Saves a numpy array as a file\n",
        "# (Each weight- and biaslayer is saved as a numpy array)\n",
        "def filesave(name,matrix,printpath):\n",
        "    if printpath == True:\n",
        "        print('Saving: \"saved_network/'+str(name)+'.npy\"')\n",
        "\n",
        "    np.save(currentpath+\"/saved_network/\"+str(name),matrix)\n",
        "\n",
        "# Loads all the image data and labels for each element in either the training dataset or the test dataset\n",
        "def load_images_ubyte(type):\n",
        "    image_buf = np.fromfile(file=(currentpath+\"/\"+type+'_images/'+type+'-images.idx3-ubyte'),dtype=np.ubyte)[16:]\n",
        "    label_buf = np.fromfile(file=(currentpath+\"/\"+type+'_images/'+type+'-labels.idx1-ubyte'),dtype=np.ubyte)[8:]\n",
        "\n",
        "    num_images = len(label_buf)\n",
        "\n",
        "    imagedata = image_buf.reshape(num_images,image_size,image_size,1)\n",
        "    rawdata = image_buf.reshape(num_images,image_size*image_size,1)\n",
        "    rawdata = np.divide(rawdata,255)  # omformer dataen fra 8-bit værdier (altså mellem 0 og 255) til floats mellem 0 og 1\n",
        "    return(rawdata,imagedata,label_buf,num_images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doxY0t7yriTu"
      },
      "source": [
        "# Funtions for initializing layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uCwpqZSCrzMD"
      },
      "outputs": [],
      "source": [
        "# Returns the names of the dynamic layers. They are named \"weightlayer1\", \"...2\" and so on. (same for bias layers)\n",
        "def namelist():\n",
        "    namelist = []\n",
        "    for i in range(hidden_layers+1):\n",
        "        namelist.append(\"weightlayer\"+str(i+1))\n",
        "    for i in range(hidden_layers+1):\n",
        "        namelist.append(\"biaslayer\"+str(i+1))\n",
        "    return(namelist)\n",
        "\n",
        "# Returns the sizes of the different layers.\n",
        "def layersizes():\n",
        "    layer_sizes = [0]*(hidden_layers+2)\n",
        "    layer_sizes[0] = image_size*image_size      # The first layer has the input size (786)\n",
        "    layer_sizes[-1]= output_size                # The last layer has the output size (10)\n",
        "    for i in range(hidden_layers):\n",
        "        layer_sizes[i+1] = hiddenlayer_size     # All the other layers get the size defined by the hiddenlayer_size defined in the start of the script\n",
        "    return(layer_sizes)\n",
        "\n",
        "\n",
        "\n",
        "# Initializes the layers, while double checking for errors (such as wrong layer sizes)\n",
        "def initialize_layers():\n",
        "    name_list = namelist()\n",
        "    if reset_network == False:\n",
        "        loaded = 0\n",
        "        found = []\n",
        "        missing = []\n",
        "        for name in name_list:\n",
        "            try:\n",
        "                path = os.path.join(currentpath,\"saved_network/\"+name+\".npy\")\n",
        "                globals()[name] = np.load(path)\n",
        "                loaded +=1\n",
        "                found.append(name)\n",
        "                print('Loaded: \"saved_network/'+name+'.npy\"')\n",
        "            except:\n",
        "                print('No array saved at location: \"saved_network/'+name+'.npy\"')\n",
        "                missing.append(name)\n",
        "\n",
        "        # The following tells the user if the saved network has less layers than specified\n",
        "        if loaded != (hidden_layers+1)*2:\n",
        "            if loaded%2 == 0:\n",
        "                print(\"\\nThe saved network has another dimension than\",hidden_layers,\"hidden layers.\")\n",
        "                print(\"The following arrays was found:\",found)\n",
        "                print(\"However, the following arrays was missing:\", missing)\n",
        "                print('Either change \"hidden_layser\" to the correct size or set \"reset_network\"=True. Please note the latter will delete and reset all current weights and biases.')\n",
        "                exit()\n",
        "            else:\n",
        "                print('\\nThe savedata is missing one or more arrays. Please reset the network by setting \"reset_network\"=True. Please note that this will delete the rest of the current savedata.')\n",
        "                exit()\n",
        "\n",
        "        # The following tells the user if the saved network has more layers than specified\n",
        "        elif globals()[name_list[-1]].shape[0] != 10:\n",
        "            print(\"\\nThe saved network has more dimensions than the \",hidden_layers,\"hidden layers specified.\")\n",
        "            print(\"Either change the amount of hidden layers to the correct amount or reset the network.\")\n",
        "            exit()\n",
        "\n",
        "\n",
        "        # Checks if the hiddenlayer_size is the same as the saved network\n",
        "        elif hidden_layers>0 and globals()[name_list[-2]].shape[0] != hiddenlayer_size:\n",
        "            print(\"The saved network has hidden layers with the size of\", globals()[name_list[-2]].shape[0], \"neurons per layer.\")\n",
        "            print(\"The current setting has\", hiddenlayer_size,\"neurons per layer.\")\n",
        "            print(\"Either change the hiddenlayer_size variable or set reset_network to True.\")\n",
        "            exit()\n",
        "\n",
        "        else:\n",
        "            print()\n",
        "\n",
        "    # Reset the network and initializes the weights randomly and the biases as 0\n",
        "    elif reset_network == True:\n",
        "        layer_sizes = layersizes()\n",
        "        for i in range(len(layer_sizes)-1):\n",
        "            globals()[name_list[i]] = np.random.randn(layer_sizes[i+1], layer_sizes[i]) * np.sqrt(2 / layer_sizes[i]) # weights\n",
        "            globals()[name_list[-i-1]] = np.zeros((layer_sizes[-i-1], 1))            # bias\n",
        "\n",
        "    else:\n",
        "        print('\\nThe variable \"reset_network\" has a wrong assigned value. It should either be True or False. Please note that \"True\" will delete and reset all the current assigned weights and biases.')\n",
        "        exit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gu6XUsWsSUD"
      },
      "source": [
        "# Math functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D7lxdKX-sWKj"
      },
      "outputs": [],
      "source": [
        "# ReLU\n",
        "def ReLU(x):\n",
        "    return(np.maximum(0,x))\n",
        "\n",
        "# The derived ReLU function\n",
        "def ReLU_d(x):\n",
        "    return(np.where(x > 0, 1,0))\n",
        "\n",
        "# The softmax function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z))\n",
        "    return(exp_z / np.sum(exp_z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBPDJgXsbl5"
      },
      "source": [
        "# Forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kk5Rm78ysgYy"
      },
      "outputs": [],
      "source": [
        "# Goes forward in the network.\n",
        "# If the global-functions are confusing look at the ...simple.py version of the script.\n",
        "# In that version the functions are written for each layer instead of a generel expression\n",
        "def forward_propagation(a1):\n",
        "    a = a1\n",
        "    for i in range(hidden_layers+1):\n",
        "        bias = globals()[\"biaslayer\"+str(i+1)]\n",
        "        weights = globals()[\"weightlayer\"+str(i+1)]\n",
        "        z = np.dot(weights,a)+bias\n",
        "        a = ReLU(z)\n",
        "        globals()[\"z\"+str(i+2)] = z\n",
        "        globals()[\"a\"+str(i+2)] = a\n",
        "\n",
        "    # the last layer doesnt use ReLU so overwrites with softmax activation instead\n",
        "    globals()[\"a\"+str(hidden_layers+2)] = softmax(z)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywel7kBosimL"
      },
      "source": [
        "# Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yLaWeyi3snsT"
      },
      "outputs": [],
      "source": [
        "# Again: if the global-functions are confusing look at the ...simple.py version of the script.\n",
        "# In that version the functions are written for each layer instead of a generel expression\n",
        "#\n",
        "# The backpropagation function is created to accomodate the dynamic layers\n",
        "# It finds the derivative of the deviation of the intended output and targeted\n",
        "# output to find the gradient and takes a step in the direction of steepest descent.\n",
        "# The step size is determined by the learning rate.\n",
        "# (however, the learningrate gets multiplied in the apply_backpropagation function to\n",
        "# eliminate batchsize-1 multiplications for each batchsize)\n",
        "def backpropagation(targetlist,a1):\n",
        "    globals()[\"dz\"+str(hidden_layers+2)] = targetlist - globals()[\"a\"+str(hidden_layers+2)]\n",
        "\n",
        "    for i in range(hidden_layers):\n",
        "        dz = globals()[\"dz\"+str(hidden_layers+2-i)]\n",
        "        ind = str(hidden_layers+1-i)\n",
        "        globals()[\"db\"+ind] = dz\n",
        "        globals()[\"dw\"+ind] = np.dot(dz,(globals()[\"a\"+ind]).transpose())\n",
        "        globals()[\"dz\"+ind] = np.dot(globals()[\"weightlayer\"+ind].transpose(),dz)*ReLU_d(globals()[\"z\"+ind])\n",
        "\n",
        "    globals()[\"db1\"] = globals()[\"dz2\"]\n",
        "    globals()[\"dw1\"] = np.dot(globals()[\"dz2\"],a1.transpose())\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F909zaGysq2k"
      },
      "source": [
        "# Applying backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SjVjcKu5svWD"
      },
      "outputs": [],
      "source": [
        "# Resets all the changes in weights to zero as they have been applied.\n",
        "def reset_weights():\n",
        "    layer_sizes = layersizes()\n",
        "    for i in range(hidden_layers+1):\n",
        "        globals()[\"w\"+str(i+1)] = np.resize([[float(0)]],(layer_sizes[i+1],layer_sizes[i]))\n",
        "        globals()[\"b\"+str(i+1)] = np.resize([[float(0)]],(layer_sizes[i+1],1))\n",
        "\n",
        "# Takes the intended changes in the weights and biases and applies them to the weight and bias layers\n",
        "def apply_backpropagation():\n",
        "    for i in range(hidden_layers+1):\n",
        "        globals()[\"weightlayer\"+str(i+1)] += globals()[\"w\"+str(i+1)]*learning_rate/batchsize\n",
        "        globals()[\"biaslayer\"+str(i+1)] += globals()[\"b\"+str(i+1)]*learning_rate/batchsize\n",
        "    reset_weights()\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSRZt2Eus1MD"
      },
      "source": [
        "# Evaluating network on evaluation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bUg7Rr3es6Ss"
      },
      "outputs": [],
      "source": [
        "# Evaluates the neural network with the test data.\n",
        "# This is the actual score of the neural network, and does not necessarily\n",
        "# correlate with the loss graph. This is a necessary step to ensure that the neural\n",
        "# network isn't overfitting to the training data. When this score drops it's an indicator\n",
        "# that the network has hit the overfitting point and its time to stop the training.\n",
        "def evaluate():\n",
        "    rawdata, imagedata, labelbuf, num_images = load_images_ubyte(\"test\")\n",
        "    loss = 0\n",
        "    for i in range(num_images):\n",
        "        a1 = rawdata[i]\n",
        "        forward_propagation(a1)\n",
        "        target_number = labelbuf[i]\n",
        "        guess = np.argmax(globals()[\"a\"+str(hidden_layers+2)])\n",
        "\n",
        "        if guess !=target_number:\n",
        "            loss +=1\n",
        "            if show_wrong == True:\n",
        "                print(\"Guess:\", guess)\n",
        "                print(\"Correct answer:\", target_number)\n",
        "                show_image(a1, guess, target_number)\n",
        "\n",
        "    evaluating_loss = loss/num_images\n",
        "    classify_percent = round((1-evaluating_loss)*100,3)\n",
        "    return(evaluating_loss,classify_percent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cfGVByis7T8"
      },
      "source": [
        "# Function training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xUIt7AlAtEAo"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    print_lines = [\"\"]\n",
        "    loss = 0\n",
        "    percentage = 0\n",
        "\n",
        "    # self explanatory\n",
        "    initialize_layers()\n",
        "    # initilizes all the weight and bias changes as 0 (so the vars exist)\n",
        "    reset_weights()\n",
        "\n",
        "    # loads all the training images and labels\n",
        "    rawdata, image_array, labels, num_images = load_images_ubyte(\"train\")\n",
        "\n",
        "\n",
        "    # The script simply loops through. It can be stopped after each iteration as there are\n",
        "    # checkpoints after each epoch\n",
        "    for epoch in range(1,epochs+1):\n",
        "        loss_list = []\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # \"Splits\" the data up so it knows how many times it can loop\n",
        "        # through the dataset with the given batchsize\n",
        "        for x in range(num_images//batchsize):\n",
        "            # the following 4 lines is just the updating progressbar for each epoch\n",
        "            percent_ = (x + 1) * batchsize / num_images\n",
        "            fractions = round(percent_ * str_len)\n",
        "            expected_remaining = (time.time() - epoch_start) * ((1/percent_) - 1)\n",
        "\n",
        "            bar = f\"[{'#' * fractions}{'.' * (str_len - fractions)}]\"\n",
        "            percent_display = round(percent_ * 100, 1)\n",
        "            runtime = round(time.time() - epoch_start, 2)\n",
        "            remaining = round(expected_remaining, 1)\n",
        "\n",
        "            # Clear and reprint updated line\n",
        "            clear_output(wait=True)\n",
        "            print_lines[-1] = (f\"Epoch {epoch}/{epochs}:\\t{bar} {percent_display}%\\t  Runtime: {runtime}s  \\tExpected remaining: {remaining}s\")\n",
        "            print(\"\\n\".join(print_lines))\n",
        "\n",
        "                # Then loops through each item in the given batch\n",
        "            for item in range(batchsize):\n",
        "                a1 = rawdata[x*batchsize+item]\n",
        "\n",
        "                # First doing forward propagation\n",
        "                prediction_ = forward_propagation(a1)\n",
        "                target_number = labels[x*batchsize+item]\n",
        "                guess = np.argmax(globals()[\"a\"+str(hidden_layers+2)])\n",
        "                if guess != target_number:\n",
        "                    loss +=1\n",
        "                targetarray = np.resize(0, (10,1))\n",
        "                targetarray[target_number] = 1\n",
        "\n",
        "                # And then backwardspropagation where it saves the changes that should\n",
        "                # be made to the weights and biases\n",
        "                backpropagation(targetarray,a1)\n",
        "                for layer in range(hidden_layers+1):\n",
        "                    ind = str(layer+1)\n",
        "                    globals()[\"w\"+ind] += globals()[\"dw\"+ind]\n",
        "                    globals()[\"b\"+ind] += globals()[\"db\"+ind]\n",
        "\n",
        "            # After each batch the sum of weight-changes gets applied\n",
        "            apply_backpropagation()\n",
        "            # and the loss in the batch gets added to the plot if its turned on\n",
        "\n",
        "\n",
        "        # Saves the network as a checkpoint so that in the case scritps gets killed,\n",
        "        # the only progress lost is just the current unfnished epoch\n",
        "        savenetwork(printpath=False)\n",
        "\n",
        "        # Evaluate the neural network against the evaluation dataset to check its actual performance\n",
        "        prev_percentage = percentage\n",
        "        loss_, percentage = evaluate()\n",
        "\n",
        "        # Prints the training time for the epoch and the classifying percentage for the neural network\n",
        "        clear_output(wait=True)\n",
        "        print_lines[-1] = (f\"Epoch {epoch}/{epochs}\\t[{fractions*'#'+(str_len-fractions)*'.'}] {round(percent_*100,1)}%\\t  Runtime: {round(time.time()-epoch_start,2)}s  \\tClassifying percentage: {percentage}%     \\tNetwork saved✔️\")\n",
        "\n",
        "        print(\"\\n\".join(print_lines))\n",
        "        print_lines.append(\"\")\n",
        "\n",
        "        # If the neural networks begins to perform worse on the evaluation dataset\n",
        "        # it's an indication that the script is overfitting to the training data\n",
        "        # That means the neural network is beginning to remember the dataset\n",
        "        # instead of learning patterns in the data.\n",
        "        # There is no workaround for this phenomenon other than stopping training\n",
        "        if prev_percentage>percentage:\n",
        "            warning_string = \"⚠️   The classification percentage has dropped. This could be a sign of overfitting. It's recommended to stop the training if it continues dropping.\"\n",
        "            print(warning_string)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyFgI8LaqQEx"
      },
      "source": [
        "#Declaring variables (Change these to modify the network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vTy-9sG_qX6Y"
      },
      "outputs": [],
      "source": [
        "from types import FrameType\n",
        "from IPython.display import clear_output\n",
        "\n",
        "### Fundamental variables\n",
        "image_size = 28          # ‼️ Shouldn't be changed - Read note:          Should only be changed if the dataset is changed to another one with other image dimensions\n",
        "output_size = 10         # ‼️ Shouldn't be changed - Read note:          Shouldn't be changed for this dataset. One output node for each number (0-9). If another dataset is used it should match the number of categories and each category (eg. shirt) should be assigned a category number (eg shirt=2)\n",
        "currentpath = os.path.join(os.getcwd(),\"Neural-network\")                 # needed to be able to find the path of subfiles\n",
        "\n",
        "### Functional variables\n",
        "reset_network = True     # ✔️  Can be toggled.  Note: it will erase the current training and reset all the weights and biases.\n",
        "hidden_layers = 2        # ✔️  Can be changed.  There can be any amount of hidden layers. However, if the value is changed the network has to be reset first. That is done by setting the variable \"reset_network\" to True\n",
        "hiddenlayer_size = 32    # ✔️  Can be changed.  Note that the size of the hidden layers can currently only be uniform. In other words, its impossible to have the first hidden layer with 32 nodes and the next with 64\n",
        "learning_rate = 0.05     # ✔️  Can be changed.  It's a very narrow window that works for the network. If changed it's recommended to set plot_graph to True to see the effect it has.\n",
        "batchsize = 100          # ✔️  Can be changed.  The amount of images the networks sees before changing it's weights. The higher, the more precise the adjustments will be, but will come at the cost of needing a higher learning rate of more epochs\n",
        "epochs = 10              # ✔️  Can be changed.  The amount of times the images (all 60.000) gets cycled through. Be aware that too many epochs will result in overfitting which makes the network worse on unseen data.\n",
        "\n",
        "### \"Cosmetic\" variables - (doesn't have any effects on the neural network itself, but can make the script slower.)\n",
        "str_len = 20              # ✔️  Can be changed.  The size of the progress bar in the terminal\n",
        "show_wrong = False        # ‼️  Doesn't work in Colab. Download the script on github instead.  Shows the predictions that the network gets wrong in the evalutation-dataset\n",
        "# If you want to see the loss graph that visualizes the training progress, then download the .py scripts on github\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZxnBuxJtO-G"
      },
      "source": [
        "# Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV0ztg86tQ6y",
        "outputId": "332823b3-c2d5-4d3e-923e-6f56e0cf5348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\t[####################] 100.0%\t  Runtime: 19.33s  \tClassifying percentage: 90.76%     \tNetwork saved✔️\n",
            "Epoch 2/10\t[####################] 100.0%\t  Runtime: 20.7s  \tClassifying percentage: 92.16%     \tNetwork saved✔️\n",
            "Epoch 3/10\t[####################] 100.0%\t  Runtime: 22.45s  \tClassifying percentage: 93.24%     \tNetwork saved✔️\n",
            "Epoch 4/10\t[####################] 100.0%\t  Runtime: 25.86s  \tClassifying percentage: 94.05%     \tNetwork saved✔️\n",
            "Epoch 5/10\t[####################] 100.0%\t  Runtime: 25.04s  \tClassifying percentage: 94.56%     \tNetwork saved✔️\n",
            "Epoch 6/10:\t[##..................] 10.5%\t  Runtime: 2.98s  \tExpected remaining: 25.4s\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGZHPVnK-w2C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}